
regex extraction strategy:
number-dot-whitespace-any length wildcard-dot-(multiple spaces, tab, or newline)
[0-9][0-9]?\.[ \t]{1,}(?:.|\n)*?\.(?=[ ]{2,}|\t|\n| \t| \n| [0-9])
[0-9][0-9]?\.[ \t]{0,}(?:.|\n)*?\.(?=[ ]{2,}|\t|\n| \t| \n| [0-9])
[0-9][0-9]?(?:\.|\:)[ \t]{0,}(?:.|\n)*?\.(?=[ ]{2,}|\t|\n| \t| \n| [0-9])
^[0-9][0-9]?\.[ ]?
[!\"#\ï¼„%&\'\(\)\*\+,-\./:;<=>\?@\[\\\]\^_`{\|}~]

Issues to Troubleshoot:
- Chartwell Law.pdf could not be parsed in.
- Kirwanm Spellacy, Danner.pdf
- Reynolds Parrino Shadwick P.A..pdf
- Rigdon Alexander & Ridon LLP.pdf

Potential Secondary Groups:
- GroupID 1 - received/entitled
- GroupID 5 - had/was not using
- GroupID 10 - prior/after



12/29/2022
- fix issue preventing 4 pdfs from being read in
	- Chartwell Law.pdf
	- Kirwanm Spellacy, Danner.pdf
	- Reynolds Parrino Shadwick P.A..pdf
	- Rigdon Alexander & Ridon LLP.pdf
- manually verify the requests list for each document
- rerun and send out summary statistics


12/30/2022
- merge requests into a single list
- clean the requests text


12/31/2022
- identify and remove stopwords
[' the ',' of ',' a ',' to ',' in ',' or ',' as ',' for ',' is ',' have ',' been ',' and ',' which ',' you ',' that ',' this ',' an ',' at ',' by ',' from ',' your ',' was ',' not ',' has ',' andor ',' had ',' no ',' were ',' any ',' are ',' all ',' on ',' he ',' with ',' she ',' did ']
- think through potential request grouping methods
- go back through the code and add comments where necessary
- reorganize code according to pylint


1/2/2023
- do some more research into string matching/grouping methods
- create pandas dataframe containing relevant information
- detail grouping algorithm (comment out the process in the script)
- research different similarity algorithm options (with fuzzywuzzy library)
	- ratio (exact matching)
	- token_sort_ratio (matches words irrespective of position)
	- token_set_ratio (matches words irrespective of position, ignores duplicates)
- implement first version of grouping/similarity matching algorithm
- add unique ID to dataframe for documents and requests
- when grouping requests, include the unqiue RequestID


1/3/2023
- create request grouping dataframe (GroupID, RequestID, Raw, Clean, NoStop)
- reorganize script into multiple files
	- parse_requests.pys
	- group_requests.py
- add logic for parsing in 3 additional word documents


1/4/2023
- fix request parsing regex to account for docx files
- manually verify the top 15 groupings
- tweak grouping with potential improvements
	- play with the similarity threshold
	- custom stopwords accross each document
	- you can only group requests across different documents


1/5/2023
- implement the secondary grouping algorithm (pt 1)
	- think through how to calculate secondary groups
	- comment out the secondary grouping algorithm process
	- create new file for secondary grouping script
	- abstract the functionality of group_requests.py
	- read in requests_with_groups.csv and filter out unecessary groups
	- for each group, run the requests through a stricter similarity algorithm
	- write results to requests_with_secondary_groups.csv
	- add group_secondary_requests to main.py


1/6/2023
- implement the secondary grouping algorithm (pt 2)
	- before similarity algorithm, try additional stopword pruning
		- get a unique list of words across each request
		- remove these words from every request
	- manually verify all subgroups for each group in requests_with_secondary_groups.csv


1/10/2023
- change filenames to include phase1, phase2...
- test running entire workflow before implementing the final phase to aggregate results
- create new file "aggregate_groups.py" and add to main.py
- read in requests_with_secondary_groups.csv and filter out small secondary groups
- change secondary grouping filtering to find the invalid_group_threshold FOR EACH primary group
- send phase 3 file with progress update


- save secondary filtered groups to csv and compare with requests_with_secondary_groups.csv manually to find a threshold that accurately filters redundant secondary groups
- finish aggregation of results
- add aggregate_groups.py to main.py
- pylint on aggregate_groups.py


Final Meeting
- send all csv files and github repo link
- briefly walk through each phase
- discuss thresholds
	- grouping / secondary grouping similarity thresholds
	- grouping / secondary grouping relevance thresholds